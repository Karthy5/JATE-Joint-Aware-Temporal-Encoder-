# JATE (Joint-Aware-Temporal-Encoder)
 **JATE is a custom Action Recognition model (`jate_model.pth`)**, developed entirely by the author. This involved:
*   Single-handedly creating a **custom dataset** comprising 150 videos relevant to retail scenarios.
*   Designing and training a **unique neural network architecture** (using PyTorch) specifically for recognizing actions like "reaching," "standing," and "walking".
*   *Currently authoring a research paper on this model and dataset for potential publication in a conference or journal.*
*   Link to JATE: https://github.com/Karthy5/JATE-Joint-Aware-Temporal-Encoder-
## JATE Demo
1.  **Walking:**
![alt text](https://github.com/Karthy5/JATE-Joint-Aware-Temporal-Encoder-/blob/main/jate_working_pics/Screenshot%202025-04-06%20084601.png?raw=true)
2.  **Standing:**
![alt text](https://github.com/Karthy5/JATE-Joint-Aware-Temporal-Encoder-/blob/main/jate_working_pics/Screenshot%202025-04-06%20084635.png?raw=true)
3.  **Reaching:**
![alt text](https://github.com/Karthy5/JATE-Joint-Aware-Temporal-Encoder-/blob/main/jate_working_pics/Screenshot%202025-04-06%20084658.png?raw=true)
4.  **Working Demo:**
![alt text](https://github.com/Karthy5/JATE-Joint-Aware-Temporal-Encoder-/blob/main/jate_working_pics/jate_video_demo.gif?raw=true)

This system demonstrates a practical application of computer vision and machine learning to solve real-world retail challenges, aligning with the objectives outlined in the Intel program.
